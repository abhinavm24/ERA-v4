{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required CV dependencies for this environment using uv\n",
    "!uv pip install -r /Users/amisra/dev/ERA-v4/requirements/cv.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import atexit\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def _nb_cleanup_on_exit() -> None:\n",
    "    \"\"\"Best-effort cleanup to release accelerator memory and Python objects on kernel exit.\"\"\"\n",
    "    try:\n",
    "        # Encourage Python to drop references first\n",
    "        gc.collect()\n",
    "\n",
    "        # MPS cleanup\n",
    "        try:\n",
    "            if hasattr(torch, \"backends\") and torch.backends.mps.is_available():\n",
    "                try:\n",
    "                    torch.mps.synchronize()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    torch.mps.empty_cache()\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # CUDA cleanup (in case this notebook is reused on CUDA machines)\n",
    "        try:\n",
    "            if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n",
    "                try:\n",
    "                    torch.cuda.synchronize()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    torch.cuda.empty_cache()\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Cleanup on exit encountered an issue: {e}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "# Register best-effort cleanup when kernel stops\n",
    "atexit.register(_nb_cleanup_on_exit)\n",
    "print(\"Registered kernel atexit cleanup handler.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    torch.backends.mps.is_available()\n",
    "), \"MPS acceleration not available on this Mac. Ensure PyTorch with MPS support is installed and Metal is enabled.\"\n",
    "print(\"MPS acceleration available: True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick tensor sanity check on MPS\n",
    "a = torch.randn(1024, 1024, device=device)\n",
    "b = torch.randn(1024, 1024, device=device)\n",
    "c = a @ b\n",
    "print(f\"Matmul successful on {c.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "x = torch.rand((10000, 10000), dtype=torch.float32)\n",
    "y = torch.rand((10000, 10000), dtype=torch.float32)\n",
    "x = x.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "x = torch.rand((10000, 10000), dtype=torch.float32)\n",
    "y = torch.rand((10000, 10000), dtype=torch.float32)\n",
    "x = x.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import multiprocessing\n",
    "import os\n",
    "import signal\n",
    "\n",
    "try:\n",
    "    from IPython import get_ipython  # type: ignore\n",
    "except Exception:\n",
    "\n",
    "    def get_ipython():  # fallback\n",
    "        return None\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def cleanup_resources(verbose: bool = True) -> None:\n",
    "    \"\"\"Free accelerator caches, terminate child workers, and run GC.\"\"\"\n",
    "    if verbose:\n",
    "        print(\"Cleaning up resources...\")\n",
    "\n",
    "    # Drop Python references and free accelerator caches\n",
    "    try:\n",
    "        gc.collect()\n",
    "\n",
    "        # MPS\n",
    "        try:\n",
    "            if hasattr(torch, \"backends\") and torch.backends.mps.is_available():\n",
    "                try:\n",
    "                    torch.mps.synchronize()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    torch.mps.empty_cache()\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # CUDA (for portability)\n",
    "        try:\n",
    "            if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n",
    "                try:\n",
    "                    torch.cuda.synchronize()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    torch.cuda.empty_cache()\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Terminate any multiprocessing children that might linger (e.g., DataLoader workers)\n",
    "    try:\n",
    "        for child in multiprocessing.active_children():\n",
    "            try:\n",
    "                child.terminate()\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    gc.collect()\n",
    "    if verbose:\n",
    "        print(\"Cleanup complete.\")\n",
    "\n",
    "\n",
    "essential_signals = {\"SIGINT\": signal.SIGINT, \"SIGTERM\": signal.SIGTERM}\n",
    "\n",
    "\n",
    "def shutdown_kernel(cleanup_first: bool = True, restart: bool = False) -> None:\n",
    "    \"\"\"Programmatically stop this Jupyter kernel (best-effort).\n",
    "\n",
    "    Tries IPython shutdown first; falls back to sending SIGTERM to self.\n",
    "    \"\"\"\n",
    "    if cleanup_first:\n",
    "        cleanup_resources(verbose=False)\n",
    "\n",
    "    try:\n",
    "        ip = get_ipython()\n",
    "        if ip is not None and hasattr(ip, \"kernel\"):\n",
    "            ip.kernel.do_shutdown(restart=restart)\n",
    "            return\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: signal this process\n",
    "    try:\n",
    "        os.kill(os.getpid(), signal.SIGTERM)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "print(\"Manual helpers ready: cleanup_resources(), shutdown_kernel().\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
